import random
import theano
import collections
import numpy

#Total number of classes we need to map our sequences to.
n_classes = 2 # For example two classes: positive or negative (or true/false)
# We can change that later when needed
n_words = 10000 # Total number of words we can support - here we are just guessing, we can get it from data later
#We represent the input sequence as a vector of integers (word id-s):
input_indices = theano.tensor.ivector('input_indices')
#We want to predict:
target_class = theano.tensor.iscalar('target_class') #e.g. could be sentiment level
#All words in the language are represented as trainable vectors:
word_embedding_size = 5    #the size of those vectors
recurrent_size = 5 

random = numpy.random.RandomState(0)
values = numpy.asarray(random.normal(loc=0.0, scale=0.1, size=(n_words, word_embedding_size)))  # E
word_embeddings = theano.shared(values, 'word_embeddings')  
input_vectors = word_embeddings[input_indices] 

values = numpy.asarray(random.normal(loc=0.0, scale=0.1, size=(  
    recurrent_size, word_embedding_size)))
w1 = theano.shared(values, 'w1')

values = numpy.asarray(random.normal(loc=0.0, scale=0.1, size=(
    recurrent_size, recurrent_size)))
w2 = theano.shared(values, 'w2')

values = numpy.asarray( 
    random.normal(loc=0.0, scale=0.1, size=(recurrent_size, recurrent_size)))  
w_output = theano.shared(values, 'w_output')  


def rnn_step(x, h_prev, w1, w2):  
    return theano.tensor.tanh(theano.tensor.dot(w2, h_prev+theano.tensor.dot(w1, x)))

context_vector, _ = theano.scan(
    rnn_step,
    sequences=input_vectors,
    outputs_info=numpy.zeros(recurrent_size),
    non_sequences=[w1, w2]
)
context_vector = context_vector[-1]

activations = [theano.tensor.dot(w_output, context_vector)] 
predicted_class = theano.tensor.argmax(activations)
output = theano.tensor.nnet.softmax(activations)[0]
cost = -theano.tensor.log(output[target_class]) 

updates = [  
    (word_embeddings, word_embeddings - .005 * theano.tensor.grad(cost, word_embeddings)),
    (w_output, w_output - .005 * theano.tensor.grad(cost, w_output)),  
    (w1, w1 - .005 * theano.tensor.grad(cost, w1)),
    (w2, w2 - .005 * theano.tensor.grad(cost, w2))
]

train = theano.function([input_indices, target_class], [cost, predicted_class], updates=updates, allow_input_downcast = True)
test = theano.function([input_indices, target_class], [cost, predicted_class], allow_input_downcast = True)

def read_dataset(path): #reading text data in the simple format: integer score, followed by a tab, followed by the text sequence
    dataset = []
    with open(path, "r") as f:
        for line in f:
         line_parts = line.strip().split("\t")
         if(len(line_parts) > 1): #convering to low case and adding spaces between any punctuation:
            s1 = line_parts[1].lower().replace(',', ' , ').replace(';',' ; ').replace(':',' : ').replace('"',' " ').replace("'"," ' ").replace("-"," - ").replace("("," ( ").replace(")"," ) ").replace("/"," / ").replace("?"," ? ").replace("!"," ! ").replace("."," . ")
            dataset.append((int(line_parts[0]), s1))
    return dataset

def create_dictionary(sentences): #this is indexing! we need to convert all words to their ID-s
    counter = collections.Counter() #Python's class that can count
    for sentence in sentences:
        for word in sentence:
            counter.update([word])

    word2id = collections.OrderedDict() #Python's class that can map words to ID-s
    word2id["<unk>"] = 0    #We reserve this for "uknown words" that we may encounter in the future
    word2id["<s>"] = 1 #Marks beginning of the sentence
    word2id["</s>"] = 2 #Marks the end of the sentence

    word_count_list = counter.most_common() #For every word, we create an entry in  'word2id'
    for (word, count) in word_count_list: #so it can map them to their ID-s
            word2id[word] = len(word2id)

    return word2id

def sentence2ids(words, word2id): #Converts a word sequence (sentence) into a list of ID-s
    ids = [word2id["<s>"],] #marks beginning of the sentence
    for word in words:
        if word in word2id:
            ids.append(word2id[word])
        else:
            ids.append(word2id["<unk>"])
    ids.append(word2id["</s>"]) #marks the end of the sentence
    return ids


path_train = "train-shuffle.txt"
path_test = "test-shuffle.txt"

sentences_train = read_dataset(path_train)
sentences_test = read_dataset(path_test)
word2id = create_dictionary([sentence.split() for label, sentence in sentences_train])
n_words = len(word2id)  # Important to set it c
assert n_words < 10000
data_train = [(score, sentence2ids(sentence.split(), word2id)) for score, sentence in sentences_train] #here we need to convert
data_test = [(score, sentence2ids(sentence.split(), word2id)) for score, sentence in sentences_test] #our data from text to the lists of ID-s

#The rest of the code is similar to the MNIST task:

for epoch in range(100):


        cost_sum = 0.0
        correct = 0
        count = 0
        for target_class, sentence in data_train:
            count += 1 
            cost, predicted_class = train(sentence, target_class)
            cost_sum += cost
            if predicted_class == target_class:
                correct += 1
        print ("Epoch: " + str(epoch) + "\tCost: " + str(cost_sum) + "\tAccuracy: " + str(float(correct)/count))
        cost_sum2 = 0.0
        correct2 = 0
        for target_class, sentence in data_test:
              cost, predicted_class = test(sentence, target_class)
              cost_sum2 += cost
              if predicted_class == target_class:
                correct2 += 1
        print ("\t\t\t\t\t\t\tTest_cost: " + str(cost_sum2) + "\tTest_accuracy: " + str(float(correct2)/len(data_test)))
